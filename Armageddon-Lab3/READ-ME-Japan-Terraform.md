# **Lab 3A â€” Execution Outline & Correct Order**

> **Principle:**  
> _Data authority first. Network second. Compute last. Verification always._

You **do not** build regions in parallel.  
You build **Tokyo first**, then **SÃ£o Paulo consumes Tokyoâ€™s outputs**.

---

## **PHASE 0 â€” Pre-Flight (Do This Before Any Terraform)**

### 0.1 Define CIDR Strategy (Write This Down)

You must lock CIDRs **before** touching Terraform.

Example (you can adjust, but donâ€™t overlap):

- **Tokyo VPC:** `10.10.0.0/16`
    
- **SÃ£o Paulo VPC:** `10.20.0.0/16`
    

Why this matters:

- TGW routing
    
- Security group rules
    
- Audit clarity
    
- Zero rework later
    

---

### 0.2 Repository & State Structure

Create the structure **now**, even if files are empty.

`lab-3/ â”œâ”€â”€ tokyo/ â”‚   â”œâ”€â”€ main.tf â”‚   â”œâ”€â”€ variables.tf â”‚   â”œâ”€â”€ outputs.tf â”‚   â””â”€â”€ terraform.tfstate (local or remote backend) â”‚ â”œâ”€â”€ saopaulo/ â”‚   â”œâ”€â”€ main.tf â”‚   â”œâ”€â”€ variables.tf â”‚   â”œâ”€â”€ data.tf â”‚   â””â”€â”€ terraform.tfstate`

**Rule:**  
ğŸ‘‰ No resource in SÃ£o Paulo may exist that depends on something not yet output by Tokyo.

---

## **PHASE 1 â€” Tokyo (Primary / Data Authority Region)**

ğŸ“ **Region:** `ap-northeast-1`

This region is the **source of truth**. Everything else depends on it.

---

### 1.1 Deploy Lab 2 Stack (Unmodified First)

Deploy **exactly** what worked in Lab 2:

- VPC
    
- Subnets
    
- ALB
    
- EC2 / ASG
    
- RDS
    
- IAM
    
- Logging
    

âœ… **Confirm before proceeding:**

- App works locally in Tokyo
    
- RDS reachable from Tokyo app tier
    
- No public DB access
    

---

### 1.2 Add Transit Gateway (Tokyo = Hub)

Now extend Tokyo.

Create:

- **Transit Gateway**
    
- **TGW Route Table**
    
- **VPC Attachment (Tokyo VPC â†’ TGW)**
    

No peering yet.

---

### 1.3 Prepare for SÃ£o Paulo (But Donâ€™t Connect It Yet)

Add **placeholders**:

- TGW route table ready for SÃ£o Paulo CIDR
    
- Security group rule allowing **SÃ£o Paulo CIDR** â†’ RDS (CIDR only, no SG reference)
    

You are preparing the corridor, not opening it yet.

---

### 1.4 Create TGW Peering Request (Tokyo â†’ SÃ£o Paulo)

From **Tokyo**:

- Create **TGW peering request**
    
- Target region: `sa-east-1`
    

This is the **only correct direction**:

> Data authority requests access â€” not the other way around.

---

### 1.5 Export Outputs (Critical)

Tokyo must expose **only what SÃ£o Paulo is allowed to know**.

`outputs.tf` should include:

- Tokyo VPC CIDR
    
- Tokyo TGW ID
    
- Tokyo TGW Route Table ID
    
- RDS endpoint (hostname only)
    

ğŸš¨ **Never expose secrets or credentials via outputs.**

---

## **PHASE 2 â€” SÃ£o Paulo (Compute-Only Region)**

ğŸ“ **Region:** `sa-east-1`

This region **consumes Tokyo**, never replaces it.

---

### 2.1 Read Tokyo Remote State

In `saopaulo/data.tf`:

- Reference Tokyoâ€™s Terraform state
    
- Import:
    
    - Tokyo VPC CIDR
        
    - Tokyo TGW ID
        
    - RDS endpoint
        

If this fails â€” **stop**.  
You donâ€™t build blind.

---

### 2.2 Deploy SÃ£o Paulo VPC (No Database)

Create:

- VPC
    
- Subnets
    
- Routing
    
- Security groups
    

ğŸš« **Explicitly do NOT create:**

- RDS
    
- Read replicas
    
- Any persistent PHI storage
    

---

### 2.3 Deploy Application Tier Only

Reuse Lab 2 app code, but:

- Point DB connection string to **Tokyo RDS**
    
- Disable any local caching of records
    
- Treat instances as disposable
    

If SÃ£o Paulo is destroyed, **nothing is lost**.

---

### 2.4 Create SÃ£o Paulo Transit Gateway

Create:

- SÃ£o Paulo TGW
    
- TGW Route Table
    
- Attach SÃ£o Paulo VPC â†’ TGW
    

---

### 2.5 Accept TGW Peering (From Tokyo)

From SÃ£o Paulo:

- Accept the TGW peering request
    
- Associate TGW route tables
    

This is the **moment the corridor opens**.

---

### 2.6 Add Routes (Both Sides)

You now add routes **symmetrically**:

**SÃ£o Paulo TGW routes**

- Tokyo CIDR â†’ TGW Peering
    

**Tokyo TGW routes**

- SÃ£o Paulo CIDR â†’ TGW Peering
    

If either side is missing â†’ traffic fails (by design).

---

## **PHASE 3 â€” Global Entry Point (CloudFront)**

### 3.1 Create Single Global Distribution

CloudFront:

- One distribution
    
- One DNS name: `chewbacca-growls.com`
    
- Origins:
    
    - Tokyo ALB
        
    - SÃ£o Paulo ALB
        
- Health checks determine routing
    

---

### 3.2 Enforce Compliance Controls

CloudFront config:

- TLS termination
    
- WAF enabled
    
- Cache only static assets
    
- `Cache-Control: no-store` for PHI endpoints
    

CloudFront **never** stores patient data.

---

## **PHASE 4 â€” Verification & Proof (This Is Mandatory)**

### 4.1 Functional Proof

From SÃ£o Paulo EC2:

- Connect to Tokyo RDS
    
- Read/write records
    
- Confirm latency but success
    

---

### 4.2 Structural Proof

In AWS Console:

- TGW attachments visible in both regions
    
- TGW peering status = available
    
- Route tables show cross-region CIDRs
    

---

### 4.3 Compliance Proof

Demonstrate:

- No RDS in SÃ£o Paulo
    
- No snapshots or backups outside Tokyo
    
- No PHI cached at edge
    
- All access logged
    

---

## **PHASE 5 â€” Explain It (Senior Skill Check)**

You should be able to say:

> â€œWe separated data authority from compute locality.  
> Tokyo owns persistence. SÃ£o Paulo provides access.  
> Transit Gateway creates a controlled, auditable corridor.  
> CloudFront delivers global access without global storage.â€

If you can explain **why each restriction exists**, you pass.

---

## **Correct Build Order (One-Line Summary)**

**Tokyo core â†’ Tokyo TGW â†’ Outputs â†’ SÃ£o Paulo VPC â†’ SÃ£o Paulo app â†’ SÃ£o Paulo TGW â†’ Peering â†’ Routes â†’ CloudFront â†’ Verification**
